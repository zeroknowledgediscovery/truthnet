% what does 90% sensitivity mean? nothing remove
% add totla in Table 1
\documentclass[onecolumn,10pt]{IEEEtran}
\let\labelindent\relax
\usepackage{enumitem}
\input{preamble.tex}
\usepackage{float}
% \usetikzlibrary{decorations.pathreplacing,calligraphy}
\input{customdef}
\usepgfplotslibrary{colormaps} % Include the colormap library
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{groupplots}

\renewcommand{\baselinestretch}{.97} 
% \externaldocument[SI-]{SI}
% \externaldocument[main-]{main}
% \externaldocument[EXT-]{exfig}
\newif\iftikzX
\tikzXtrue
\tikzXfalse 
\newif\ifFIGS
\FIGSfalse  
\FIGStrue
\cfoot{\scriptsize\thepage}
\lfoot{}
\rfoot{} 
\def\EXTENDEDDATA{Extended Data\xspace}
\def\SUPPLEMENTARY{Supplementary\xspace}
\def\Methods{Online Methods}
\def\SPREFIX{S-}
\def\SPREFIX{}
\def\SUPPLEMENTARY{Extended Data\xspace}
\pgfplotsset{compat=1.16}
\def\Methods{Methods\xspace}
\def\vrts{\texttt{VeRITAS}\xspace}
\def\Mpos{M^+}
\def\Mcont{M^0}
\def\x{x}
\def\y{y}

\begin{document}
\maketitle

{\bf \sffamily \fontsize{10}{12}\selectfont \noindent   
  {\normalfont \itshape Abstract:} Exaggerating or faking  the symptoms of a mental health disorder  can confound structured diagnostic interviews and hinder clinical psychiatric assessments~\cite{Rogers1997,rogers2008clinical}.  We introduce an  artificial intelligence (AI) framework for detecting symptom fabrication in mental health assessments, illustrated here for Post-Traumatic Stress Disorder (PTSD) diagnoses, for which malingering is a known problem~\cite{frueh2007us,taylor2007detection}, partially ascribable to the  potential for secondary financial gain from positive diagnoses.  Algorithm \vrts employs novel generative AI to infer statistical dependencies inherent in true response patterns, and flags  responses which violate these subtle  constraints.  With a study sample of $n=651$ patients, \vrts is estimated to have a   Area Under the Curve (AUC) of $\geqq 0.95\pm 0.02$, with  sensitivity $>95\%$, specificity $>88\%$ respectively, and positive likelihood ratios between $9.9 - 19.77$ achievable based on the population prevalence of malingering in the context of PTSD diagnosis. We show that in our methodology having training in forensic psychiatry, or other relevant mental health experience, is not helpful in deceiving  the algorithm. Our tool   offers an  objective, disease-specific, fast (average time $\leqq 4$ min)  approach to detect fake  PTSD, and if adopted, can  ensure that healthcare resources and disability concessions reach  those genuinely in need,  while  helping to maintain  integrity of clinical data. Moreover, the ability to identify and help  patients who might be malingering due to other mental health conditions, poverty or   socio-economic compulsions can  improve general health outcomes  in disadvantaged communities.  
}

\vspace{10pt}


\section*{One Sentence Summary}
Generative AI-based lie-detector flags fake mental health symptoms with 95\% accuracy immune to deception even by trained psychiatrists.

\section*{Introduction}
Diagnosis and measurement of severity of mental health disorders  typically  rely  on structured interviews~\cite{ali2015multimodal}, clinician symptom ratings, or patient self-reports, and are potentially susceptible to  intentional fabrication of  symptoms~\cite{Rogers1997,rogers2008clinical},  referred to as ``malingering''. In the  context of the measurement and diagnosis of Post-Traumatic Stress Disorder (PTSD) as an example,  subjective stressors coupled similarity in presentation,   relatively easy access to information on  how to fake PTSD,  and financial  incentives related to a positive diagnosis  in  legal procedures or disability claims, is known to hinder accurate  clinical assessments.  While there are existing elementary approaches to construct ``lie scales'' dating back to the Minnesota Multiphasic Personality Inventor (MMPI)~\cite{butcher2010minnesota}, here, we present an approach based on a novel generative artificial intelligence (AI) model to flag malingering suspects rapidly, efficiently and accurately.

We illustrate use of our methodology using symptom-level data for PTSD. Clinically, PTSD is an anxiety disorder that can develop after experiencing a traumatic event. In the United States,  disability compensation is   available for  those with mental health disorders, which while being a crucial resource for the truly afflicted, incentivizes malingering~\cite{frueh2007us,taylor2007detection}.  Thus,  faking PTSD to access medical treatment, commit insurance, personal injury and other frauds, or in an attempt to evade criminal liability and penalties~\cite{guriel2003assessing,salloway1990opiate,resnick2008malingering,burkett1998stolen} is unfortunately not rare. While  PTSD is a serious mental health condition  associated with  substance use disorder, mood disorder, anxiety disorder,  personality disorder, increased morbidity,  and possibly with increased  mortality~\cite{goldstein2016epidemiology,schnurr2009posttraumatic}, false diagnoses and fabrication of self-reported symptom severity can cause substantial financial drain~\cite{lopiccolo1999current, oboler2000disability} to  healthcare systems, divert crucial resources from where they are needed~\cite{taylor2006clinician}, and interfere with study outcomes by introducing inaccuracies in clinical data~\cite{rosen2006dsm}. Accurate disambiguation of true and fabricated PTSD is therefore of high importance, especially with sources suggesting that over 20\% of  personal injury cases, as well as  20\%  of the Veterans seeking combat compensation could be  fabricating their condition~\cite{marx2011ptsd,rogers1994explanatory,leeshaley1997mmpi2,frueh2007us}. In addition to curbing  financial fraud, ability to flag such incongruities can help address the myriad of   factors that can drive malingering behavior, including other mental health conditions, a lack of access to healthcare~\cite{park2021race,muntaner2004socioeconomic} and  an inability to seek help arising from poverty and other broad-ranging socio-economic conditions.

\input{figuresandtables}

Despite the general difficulty in formulating principles to detect malingering~\cite{drob2009clinical}, multiple standardized tests~\cite{Wong2005,smith1997detection} and validity assessment tools~\cite{ben2012interpreting} have been proposed, with limited success. These tools typically aim to incorporate patterns observed in diagnostic populations that might disambiguate fabricated symptoms from real ones or ask similar or related questions multiple times to verify consistency. However,  existing approaches do not target specific disorders, almost always require expert interpretation, are  subjective, and by design are unlikely to be effective against a malingerer with psychiatric training (See Table~\ref{tbl:methods_summary}). Other  strategies with physiological monitoring and linguistic analysis~\cite{ekman1991who,mihalcea2009lie,burgoon2008cognitive,zhou2004automating}  cannot be easily adopted in structured or semi-structured interviews or patient reported outcome (PRO) measurement.

In  \vrts we leverage the fact that that both clinician-rated and patient self-reported symptoms have  statistical dependencies  arising from the nature of the questions themselves (Fig.~\ref{figscheme}) and are modulated by the latent trait or condition we are attempting to measure $e.g.$, PTSD diagnosis and/or severity. We operationalize this principle without requiring human-understanding of the specific items being administered; thus, making the approach  specific to the disease at hand (PTSD), while being potentially generalizable to other disorders if appropriate training data are available.

Our key finding is that the subtle cross-dependencies between the symptom items are challenging to  mimic on-the-fly, even with training in forensic psychiatry. In particular, maintaining the right amount of expected structure in a sequence of responses to a structured diagnostic interview, measurable  via our  ``complexity'' and ``surprise'' parameters proves is  difficult.   Thus,  \vrts offers a robust  approach for flagging malingering subjects, can be trained to target specific disorders,  can be administered in less that $4$ minutes on average, and requires   no  subjective interpretation.

\section*{Results} 
\subsection*{Participants and Data Sources}
Our  first   dataset  (referred to as the VA dataset)  comprises  {\color{Red1}$n=304$} participants  recruited at a Veterans Health Administration facility for an earlier study~\cite{brenner2021development}. Veterans  between the ages of 18 and 89 years were recruited with written informed consent. Once eligibility was determined by the study team, participants completed a PTSD-symptom questionnaire from the CAT-PTSD item bank~\cite{gibbons2012development,brenner2021development}, comprising $211$ items, including some items from the  PTSD Checklist (PCL-5). Participants  were also interviewed using the Clinician-Administered Scale for PTSD for Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), which resulted in $86$ PTSD  diagnoses, and $218$ participants  deemed as not having PTSD. We used {\color{Red1} 60\%} of the data for inferring our generative models, and the rest were used for validation, including determining the out-of-sample AUC for identifying PTSD vs no-PTSD cases. The possibility of malingering was not recorded in this dataset.

Our second dataset  (the PL dataset) comprises results from online surveys conducted by a third party vendor (Prolific) hired by the study team. Referred to here as the Prolific dataset, it comprises {\color{Red1}$n= 310$} participants (200 in the US, and 110 in the UK), screened for the absence of past or present mental health diagnoses. The participants were asked to fake symptoms of PTSD while taking the \vrts interview.

Our third dataset (the PS cohort) comprises {\color{Red1} $n=27$} mental health professionals including   forensic psychiatrists  recruited from academic institutions in the Chicago region (See Table~\ref{tabpsych}) who, akin to the PL participants, took the \vrts test aiming to malinger and get a false PTSD diagnosis. The objective in collecting the PS cohort was to test if extensive knowledge of the relevant mental health pathologies makes it easier to defeat the test.

In total, we considered {\color{Red1} $n=651$} participants comprising  US Veterans,  general population in the US and the UK, and  mental health professionals, with broad representation across sexes, race and ethnicity.   Average completion times and detailed demographic composition of the respondents in the PL dataset  are shown in Table~\ref{tabprolific}, and that for the PS cohort are shown in Table~\ref{tabpsych}. Both the PL and the PS datasets were generated via   a ``web-app'' implementation of the \vrts. Unlike the PL dataset for which identifying respondent information is available to the third-party vendor (but not released to the study team), for the PS cohort we only collected de-identified responses, and hence, while we knew who were in the complete set of respondents for the PS cohort, we did not collect  information to identify which response was generated by which individual.


\subsection*{Principle of Characterizing Malingered Responses}
Our first insight behind \vrts  is that  response sequences from malingering subjects necessarily  have high average ``surprise'', $i.e. $ deviate more on average from the context-specific model predictions of symptom ratings ($i.e. $ item responses). Here context-specificity refers to the dependence of a response to  responses to other items, which might be indicative of behavioral  or mental health phenotypes. Our other insight, based on observations,  is that attempts  to mimic true PTSD   tends to generate ``over-structure'' compared to responses from participants with actual PTSD, $i.e.$, malingering tends to manifest  too much regularity in the response patterns. In other words, mimicked responses are less ``complex''. Here, we understand complexity in the formal sense of Kolmogorov~\cite{li2008introduction}: more complex objects are less compressible, and  perfectly random sequences being not significantly compressible at all. Thus, a sequence of all 1's ($i.e. $ symptom absence) is very compressible (since, instead of storing individual responses, one could just remember such responses as ``all 1's''), but a sequence generated from the sequential tosses of a fair coin is not very compressible, $i.e.$ has less structure.  We hypothesize that true response sequences  tends to have maximal randomness, conditional on being constrained by the emergent cross-dependencies. In other words, true responses  should have just the right amount of identifiable structure, and \textit{no more}.  More structure will make it less random, and less or inappropriate structure will increase surprise. Thus too much, too little or deviant structures are all indicative of deviant responses, interpreted here as probably malingering.  It is important to note that  these criteria (upper bound on surprise and lower bound in complexity) are based solely on algorithmic properties of the response sequence, and do not require human  understanding of the items themselves in a natural-language sense. Instead, for each response sequence $x$, we compute two quantities: $\kappa(x)$ and  $\nu(x)$, referred to as the complexity  and the surprise parameters respectively (See \Methods for precise definitions). Kolmogorov complexity is not computable~\cite{li2008introduction}, and thus we estimate a related computable parameter (See section on \vrts analysis in \Methods).

The distributions for  $\kappa, \nu, \mu$   are characterized from the part of the  VA data used for training, despite the fact that this dataset does not have  designation of ``malingering''. This is possible, because these quantities are computable from just the response sequences themselves.  Concretely, we propose  a response sequence $\x$  should be flagged as an instance of malingering if for  suitably chosen thresholds $\mu_0,\nu_0, \kappa_0$:
 \cgather{
  \chi(x) \triangleq  \big (  \mu(x)  \geqq \mu_0 \big ) \bigwedge \bigg ( \big  (\kappa(x) \leqq \kappa_0\big ) \vee \big (\nu(x) \geqq \nu_0\big )   \bigg ) 
  }%
This may be paraphrased as \textit{``a response sequence has high likelihood of malingering if it 1)  produces a diagnosis of PTSD with high probability, and 2) is either too surprising or too simple.''} The decision thresholds are obtained from theoretical considerations and the VA data (See \Methods), which allows us to choose thresholds as reflecting specificity-sensitivity trade-offs (Fig.~\ref{figperf} a,b).
  
\subsection*{Integrated Diagnostic Capability}

Given a sequence of responses to a diagnostic interview, our first task is to determine if a particular subject should be diagnosed with PTSD,  if the possibility of malingering is ignored. We call this a ``naive diagnosis''. This diagnostic information might be available to \vrts externally ($e.g.$ from a physician's assessment of the patients). However, \vrts also has an integrated capability for naive diagnosis: we identify separate  generative models for 1) the diagnosed set of patients in the training set ($\Mpos$, some of them might be malingering) and 2) for the patients identified as not having PTSD ($\Mcont$). Then given the sequence of responses from a new subject, we estimate if that sequence is more likely to have been generated by the model for  $\Mpos$ vs that for $\Mcont$.  We validate this diagnostic capability using the training data described above, and we achieve good disambiguation between $\Mpos$ and $\Mcont$, with out-of-sample AUC  {\color{Red1}$0.867\pm 0.008$} (See Fig.~\ref{figperf} c), which is at par or better compared to reported tools, $e.g.$,  CAT-PTSD~\cite{Brenner_2021}. In the \vrts algorithm, for a given response sequence $\x$, the naive diagnosis risk score is denoted as $\mu(x)$, and referred to as simply the ``score'' (See \Methods). Fig.~\ref{figperf},d shows the class specific distributions estimated for $\mu$ for the VA data. 

\subsection*{\vrts Validation Strategy and Performance}
Since the VA data does not indicate presence or absence of malingering, we adopted a non-standard approach for validating \vrts. We assume that the PL participants do not have PTSD (based on their screening of not having past or present mental health diagnosis, little or no anxiety severity, and being informed to not take the test if experiencing PTSD symptoms), and that all of them were attempting to malinger as directed. This allows us to  measure the false negative rate (or 1 - sensitivity), as function of the \vrts parameters. The success rates of these participants in getting a ``diagnosis'' at a average sensitivity of {\color{Red1} $94.2\%$} (using \vrts parameters $\kappa_0=1,\nu_0=0.76,\mu_0=1.35$) is shown in Table~\ref{tabprolific}. Then we check how many of the VA participants are flagged as malingering amongst the ones with a PTSD diagnosis. To determine a lower bound on performance we can assume all of these subjects are false positives (which is unlikely, but nevertheless is an upper bound on false positives as function of \vrts parameters). This allows us to construct a lower envelop of the ROC curve, and hence estimate a lower bound of the AUC for determining malingering, establishing a minimum AUC of {\color{Red1} $0.95 \pm 0.02$ at 95\% confidence}. High performance operating points for different values of the population prevalence (prevalence has been reported to be high between 20 to 30\%)~\cite{matto2019systematic}, reflecting specificity/sensitivity trade-offs are shown in Table~\ref{tabperf}. These results indicate that i fthe population prevalence in 30\%, then we can achieve {\color{Red1} $95.6 \pm 1 \%$ sensitivity with $88\%$ specificity, along with $89.4 \pm 0.1\%$ PPV and $97.9 \pm 0.4\%$ NPV, and positive likelihood ratio $9.9 \pm 0.9$. For lower population prevalences, $e.g.$ at $20\%$, we can achieve $94.2 \pm 1.2 \%$ sensitivity with $93\%$ specificity, along with $83.1 \pm 0.2\%$ PPV and $98.4 \pm 0.3\%$ NPV, and positive likelihood ratio $19.77 \pm 0.35$.} We note  that these numbers represent lower bounds on \vrts performance, due to our assuming an upper bound on false positives. The variation of the complexity and surprise parameters for the VA data, along with a set of decision thresholds, is shown in Fig.~\ref{figperf} f,g for the VA data,  panels i,j for the PL data, and panels ll,m for the PS data. The correlation matrix between $\kappa, \nu, \mu, dx, \chi$ (where dx is the variable for clinician diagnosis) for the VA dataset (panel h) shows that the complexity and the surprise parameters are well-correlated, and the dx and the $\mu$ parameters are well-correlated, and the malingering decision $\chi$ is not very well correlated to either of these. This pattern is closely replicated in the PL and the PS datasets (panels k,n respectively), where the ``dx'' variable  refers to the predicted diagnosis. The estimated lower envelop for the ROC curve for the \vrts algorithm is shown in panel o.

\subsection*{Item Presentation and Response Sequence in Administration}
In \vrts we present  a small random subset of the full item bank to the subjects. Although the models for cross-dependency are inferred using the complete response sequence to all 211 items available, presenting a smaller  item set makes the test feasible, while also producing a vast number of possible variations. In the version we validated, 20 items are presented. These are randomly generated from a distribution reflecting the  impact  of the items in the inferred models on their ability to disambiguate between responses from diagnosed PTSD patients and the control group. {\color{Red1} Impact is measured using standard SHAP analysis (See \Methods)}. SHAP-values for items are normalized to a probability mass function (pmf) over the top $r=20$ items, which is then sampled  to produce the item set. We can select items adaptively similar to CAT-PTSD~\cite{brenner2021development}; such  complications have been presently avoided. Fig.~\ref{figperf}e shows the sorted estimated SHAP values for the items, and shows the threshold of selection for presentation. 

\subsection*{\qnet Inference and Cross-talk Modeling}
The generative models for response sequences inferred separately from the PTSD-positive and PTSD-negative patients are referred to as ``\qnet{s}''. Structurally, an \qnet comprises an interdependent collection of  local predictors, each aiming to predict the  response to a specific item using as features  the responses to ther items from the item bank.     (Fig.~\ref{figscheme}). Thus,  an \qnet comprises almost as many such  position-specific predictors as the length of the response sequence. These individual predictors are implemented as conditional inference trees~\cite{Hothorn06unbiasedrecursive}, in which  nodal splits  have  a minimum pre-specified significance in differentiating the  child nodes. Thus, each predictor yields an estimated conditional response distribution  for each item. The set of items acting as features in each predictor are automatically identified, \textit{e.g.}, in the fragment of PTSD-positive  \qnet (Fig~\ref{figscheme}), the predictor for item ptsd119 includes the response to item ptsd19, that for ptsd19 includes item ptsd43, that for ptsd43 includes ptsd208, which in turn includes ptsd43, revelaing complex possibly cyclic dependencies. Owing to this recursive expansion,  a complete \qnet  captures the complexity of the rules guiding the subtle cross-talk emerging between different items in a survey, and can be trained wihout linguistic understanding of the items themselves. The \qnet{s} inferred for $\Mpos$ and $\Mcont$ have 5,852 and  10,301 independent parameters, encoding the probability distributions on the tree leaves,  which were identified automatically in the course of inferring the sets of conditional inference trees. The overall  \vrts model, which comprises these \qnet{s} along with the three thresholds described before, therefore has 16,156 parameters.

\subsection*{Time-to-completion and Malingering}
In the PL dataset, the average completion time was 3.4 minutes ([2.9, 3.9], 95\% CI). 
We find that malingering flags have no significant correlation with the time to completion. The success rate of defeating \vrts is observed to have a correlation of $0.48 ([-.27, .87], 95\% CI)$. Thus, fast responses  that might reflect poor quality survey in general~\cite{tourangeau2000psychology,malhotra2008completion,callegaro2009response} might not be effective in detecting malingering, especially if participants  are putting more thought into how to fake their symptoms.

\subsection*{Performance among Mental Health Professionals}
The PS dataset illustrates that training in forensic psychiatry or neuropsychology confers no significant
advantage in defeating the \vrts algorithm. Only one participant succeeded in not being flagged, implying a
false negative rate of $3.7\%$. Using the same settings, participants in the PL dataset (drawn from the general population in the US and the UK
as part of Prolific's cohort), were able to achieve a false negative rate of $5.8\%$, which is about 56.8\% worse compared to the cohort comprising a substantial number of participants with training in forensic psychiatry.

\subsection*{Number of Distinct Variations of \vrts Implementation}
With $r=20$ items presented, randomly chosen from the top $N=40$ shortlisted from the master item bank (sorted according to SHAP values, as described before), and each item having  $L=5$ possible responses, we can have $\binom{N}{r}=137,846,528,820$ variations of the test, with  $L^r\binom{N}{r} = 13,146,069,414,138,793,945,312,500$ or approximately $10^{25}$ possible responses, which is approximately equal to the number of stars in the observable universe. Thus, it is non-trivial for human subjects to ``learn'' or ``train'' to defeat the algorithm.

\subsection*{Comparison Against State of Art}
A well-known tool used to identify malingering in mental health diagnoses is the Structured Interview of Reported Symptoms, 2nd Edition (SIRS-2)~\cite{Wong2005}, which  has a reported performance of sensitivity of 0.80, a specificity of 0.975, and positive and negative predictive powers of more than 0.90 (based on a base rate of 31.8\%), takes 30-40 minutes to complete,  needs extensive expert interpretation, and is not disease-specific. In contrast, \vrts may be completed in under 4 minutes, can have sensitivity and specificity both above 90\%, has PPV over 86\% and NPV over 90\% in selected operating points, can be tuned to specific disorders, and may be administered automatically. The crucial difference in \vrts is the near-impossibility of defeating it through coaching, and its effectiveness in the scenario that the subject has training as a mental health professional. The principles on which existing tools such as the SIRS-2 are based makes them highly unlikely to be effective if the subject is familiar with the symptomologies of  mental disorders, and the approaches  employed to flag malingering.

\section*{Discussion}
The  \vrts algorithm introduced here aims to identify deception in clinician rated and patient self-reported symptoms associated. While we demonstrate applicability in assessments for PTSD, the underlying principles are generally applicable for detecting faked symptoms for other mental health disorders, and even more  generally for vetting adversarial responses in structured interveiews.

Malingering presents a significant challenge in accurate diagnosis of mental health disorders. Estimates of prevalence of faked symptoms for PTSD in psychiatric and criminal justice settings range from 8\% to 64\%~\cite{mcdermott2013malingering,schmidt2020base,matto2019systematic}.  This variability in reported estimates highlights the complexity of the issue and the need for reliable, rigorous, and principled methods for detecting deception in this context ~\cite{DePaulo2003}.

Detection methods for mentla health conditions, and for PTSD in particular,  must sensitively navigate the complex motivations behind their potential fabrication. These motivations extend beyond financial incentives such as disability compensation, insurance claims, or legal benefits, encompassing mental health issues and socio-economic factors that restrict access to healthcare. The presence of malingering, particularly when prevalent in certain clinics, may point to broader societal issues. Furthermore, a high incidence of malingering not only burdens healthcare and legal systems but also compromises the integrity of clinical diagnoses and research. This misalignment may result in the misallocation of resources and impede the treatment of those genuinely suffering from PTSD. Identifying such behaviors efficiently and discreetly, avoiding the stigma of a ``malingering test'', is crucial in addressing the multifaceted challenges encountered in clinical practice. Research indicates the financial and clinical significance of accurately diagnosing mental health conditions, with disability payments and the prevalence of service-connected mental disorders among veterans highlighting the issue's complexity. Moreover, the prevalence of malingering varies by context, underscoring the need for a nuanced approach to diagnosis and treatment.

Our approach represents a significant departure from traditional methods of malingering detection, which often rely on domain-specific knowledge~\cite{walczyk2018review} and standardized tests developed through extensive research on both genuine patients and malingerers (Table \ref{tbl:methods_summary}). While these traditional methods, including the SIRS~\cite{Wong2005} (requiring 30-40 minutes and expert assessments), the Structured Inventory of Malingered Symptomology (SIMS)~\cite{smith1997detection}, and validity scales associated with the Minnesota Multiphasic Personality Inventory-2~\cite{ben2012interpreting}, have shown accuracy rates in the range of 85\% to 95\% depending on the specific problem context~\cite{rogers2008determinations,sartori2008accurately,gregg2007vying,monaro2017detection}, they come with limitations. These include the requirement for substantial expertise to develop and administer, vulnerabilities to clinician bias, and the potential for false positives and negatives. Furthermore, they may not easily extend to other contexts or disorders, and their effectiveness can be compromised by coaching or prior knowledge of psychiatric symptomology.
Additionally, current practice does not have good methods to discreetly flag malingering. Attempting to make such assessments without any formal tools is problematic, since humans are 
generally poor at detecting lies, with accuracy rates often barely surpassing chance~\cite{ekman1991who}.


In contrast, \vrts leverages statistical  differences in an individual's response patterns compared to a set of baseline responses, aiming to capture the complex, often non-obvious dependencies between interview questions. The underlying model is a  nonparametric generative model that maps the inter-question relations and dependencies, enabling the detection of inconsistencies indicative of malingering. 

Our findings indicate that \vrts can achieve high sensitivity and specificity in detecting malingering, with performance metrics potentially surpassing or at least comparable to those of existing state-of-the-art techniques, but with several advantages. \vrts requires less time for administration, does not necessitate domain-specific expertise for interpretation, and minimizes the risk of bias. Moreover, its design makes it challenging for individuals, even those with psychiatric training, to defeat the system through coaching or preparation.

However, the impact of \vrts in  clinical practice must be evaluated carefully, particulalrly for unintended  ethical implications, especially with respect to  vulnerable communities within which  the impact of mental health disorders are often exacerbated by limited access to healthcare, socio-economic instability, and the stigma surrounding mental health diagnoses. A non-zero  risk of false positives  could unjustly exclude vulnerable individuals from receiving necessary care, and  the algorithm's reliance on statistical patterns may overlook the nuanced expressions of PTSD symptoms across different cultures, possibly leading to biased assessments. This underscores the importance of integrating cultural sensitivity into the algorithm's development and emphasizing the indispensable role of human judgment in the diagnostic process~\cite{DePaulo2003,ekman1991who}. Ongoing research and validation in diverse settings are imperative to refine the algorithm's application, ensuring it serves as a tool for empowerment rather than exclusion, particularly in underserved populations.



In conclusion, while malingering presents a persistent challenge in the accurate diagnosis and treatment of PTSD, our study offers a promising new direction for detection methods. By employing a sophisticated, data-driven approach that transcends the limitations of traditional methods, \vrts provides a powerful tool for clinicians and researchers. Its adoption could significantly enhance the integrity of PTSD diagnoses, ensuring that resources are allocated to those genuinely in need and supporting the broader goals of psychiatric care and research. However, further research and validation across diverse populations and settings are essential to fully realize its potential and applicability in clinical practice.

\section*{Methods}
\section{Definitions \& Notation}

\begin{defn}[Survey]
  A survey for the purpose of this work is a structured interview, consisting of a finite number of questions (items) posed to a set of participants, with these items drawn from a finite item bank, and  whose responses must be one froma pre-specified set fo choices, $e.g.$, the Likert scale, with missing values for the responses allowed. 
\end{defn}

\begin{defn}[Response vector]
  A response vector is the set of responses to a survey from a single participant, typically assuming that not all items are posed, and allows for the possibility that some responses are missing.
\end{defn}

A \qnet, as described here, is a model of the response dependency  structure for questions (items) posed to participants in a survey.  The \qnet explicitly estimates individual conditional distributions of each item response, which collectively serve as a model of the full joint distribution of the responses. 

\begin{defn}[\qnet]
  \label{def:qnet}
  Let $X \sim P$ be an $n$-dimensional discrete random vector supported on a finite set $\Sigma$ and following distribution $P$, i.e. \[X = (X_1, \ldots, X_n) \sim P, \hspace{0.25in} \operatorname{supp}(X) =  \Sigma = \displaystyle\prod_{i=1}^n \Sigma_i \hspace{.1in} \text{with } |\Sigma| < \infty.\] For $i = 1, \ldots, n$, let $P_i := P(X_i\,|\,X_j=x_j \text{ for } j \neq i)$ denote the conditional distribution of $X_i$ given the values of the other components of $X$.  Finally, for each $i = 1, \ldots, n$, let $\Phi^P_i$ denote an estimate of the distribution $P_i$.  Then the set $\Phi^P := \{\Phi^P_i\}_{i=1}^n$ is called a \emph{Quasinet (\qnet)}. Identifying the true distribution $P$ as the one describing the joint statistics of the responses from a survey with $n$ items, we also refer to  $\Phi^P$ as the \qnet for the survey $P$.
\end{defn}

When $P$ is clear from context, we may omit the superscript and simply write $\Phi = \{\Phi_i\}$ to denote the \qnet. The motivation for Definition \ref{def:qnet} is that the collection of all estimators $\Phi = \{ \Phi_i \}$ contained in a \qnet represents the set of all inferred dependencies from the observed ecosystem.  While the definition allows for arbitrary method of algorithm to construct the estimators $\Phi_i$, the utility of a \qnet clearly depends primarily on the properties of the $\Phi_i$.  In this study, we aim to minimize the set of a priori assumptions on the overall model structure to allow the complex dependencies present in $P$ to emerge. To that end, throughout this work all {\qnet}s are computed using conditional inference trees~\cite{sarda2017conditional} (a variant of classification and regression trees) to compute each $\Phi_i$. In general each \qnet component $\Phi_i$ is computed independently from the other $\Phi_j$, which allows a network structure to emerge amongst these estimators.

An important quantity for an inferred \qnet is the persistence function $\mem{\x}$.
% 
\begin{defn}[Persistence Function]\label{def:mem}
  Given a survey $P$ inducing the \qnet $\Phi^P$ and a response  vector $\x = (x_1, \ldots, x_n)$, the persistence $\mem{\x}$ of  $\x$ in the population modeled  by the \qnet:
  \cgather{
    \mem{\x}^P := \operatorname{Pr}(\x \in P) = \prod_{i=1}^n  \Phi^P_i(X_i = x_i\,|\,X_j = x_j, j \neq i) %
  }%
\end{defn}
The persistence function $\mem{\x}^P$, as the name suggests,  is the probability that $\x$ persists, $i.e.$,  $Pr(\x \rightarrow \x)$ for the population modeled by the \qnet $P$, with $1-\mem{\x}^P$ being the probability that $\x$ is altered by a  random perturbation.

We will show  that if for two inferred \qnet models $P,Q$, we have $\mem{\x}^P \geqq \mem{\x}^Q$, then it is more likely that model $P$ generated $\x$. This is an important result that justifies the definition of the
score parameter in Defn.~\ref{defvrts}.

The \qnet allows us to rigorously compute  bounds on  the probability of a spontaneous change from one response vector to another, induced by spontaneous chance variations. Not all perturbations in a vector are either likely or contextually meaningful. With an exponentially exploding number of possibilities in which a vector over a large set of items can vary, it is computationally intractable to directly model all possible dependecies; nevertheless, we can constrain the possibilities using the patterns we uncover via the \qnet construction.  A key piece of this approach is to design  an intrinsic distance (\qdist) between any two response vectors, which is reflective of this underlying  dependency structure.  

\begin{defn}[\qdist]
  \label{defqdistance} 
  Let $\Phi^P = \{\Phi_i^P\}_{i=1}^n$ and $\Phi^Q = \{\Phi_i^Q\}_{i=1}^n$ denote {\qnet}s on populations $P$ and $Q$, and suppose $\x=(x_1, \ldots, x_n)$ and $\y =(y_1, \ldots, y_n)$ are samples of $X \sim P$ and $Y \sim Q$ respectively. Then the \qdist $\theta_{P,Q}(\x,\y)$ between $\x$ and $\y$ is\[\theta_{P,Q}(\x,\y) := \frac{1}{n}\sum_{i=1}^n \left[ \mathbb{J}^{\frac{1}{2}} \left(\Phi_i^P(X_i |X_j = x_j, j \neq i) \,\|\, \Phi_i^Q(Y_i|Y_j = y_j, j \neq i\right ) \right]\]%
  where $\mathbb{J}$ denotes the Jensen-Shannon divergence~\cite{cover}.
\end{defn}

For brevity, we may write simply $\theta$ (dropping the suffixes) if the populations are clear from context.  Since the Jensen-Shannon distance $\mathbb{J}$ is a legitimate metric~\cite{fuglede2004jensen} on the set of probability distributions (unlike KL-divergence), $\theta$ inherits nonnegativity, symmetry, and respects the triangle inequality; it follows that \qdist is a (pseudo)-metric on $\Sigma$. Note that, being a pseudo-metric implies that  we may have $\theta(\x, \y) = 0$ for $\x \neq \y$, i.e. distinct vectors can induce the same distributions over each index, and thus have zero distance. This is in fact desirable, since we do  not want  our distance to be sensitive to changes that are not meaningful.  The intuition is that not all variations are equally important or likely.    Moreover, we show in Theorem \ref{thm:probbnd} that the log-likelihood of a vector $\x$ transitioning to $\y$ scales with $\theta(\x,\y)$, allowing us to directly estimate the probability of spontaneous (or sequential) jumps between abundance profiles.

\begin{thm}[Probability Bound]\label{thm:probbnd}
  Given a vector $\x$ of length $n$ from $P$ that transitions to $\y$ from $Q$, we have the following bounds at significance level $\alpha$.
  \cgather{
    \mem{y} e^{ \frac{\sqrt{8}N^2}{1-\alpha}\theta(\x,\y)} \geqq Pr(\x \rightarrow \y) \geqq \mem{\y} e^{-\frac{\sqrt{8}N^2}{1-\alpha}\theta(\x,\y)}
  }%
  where $\mem{\y}$ is the persistence of  $\y$  (Def.~\ref{def:mem}), and $\theta(\x,\y)$ is the q-distance between $\x,\y$ (Def.~\ref{defqdistance}).
\end{thm}

\begin{proof}
  See later in Section~\ref{secproof}.
\end{proof}

Theorem 1 gives theoretical backing to the claim that samples generated by the \qnet indeed reflect likely perturbation possibilities from the current state.  Thus we can use the \qnet to draw contextually realistic samples that respect the  cross dependencies and reduce surprise  (that is, the \qnet-inferred conditional distributions can be used to  generate approximate samples from the population $P$). 

\begin{rem}[Neighborhood Structure]\label{rem:neighborhood}
  It follows from Th.~\ref{thm:probbnd} that we have for some constant $C$,
  \cgather{
    \ln \left \vert \frac{Pr(x \rightarrow y)}{Pr( y \rightarrow y)} \right \vert \leqq C \theta(x,y)
  }
  implying for all response vectors $y$ within a small neighborhood of $x$ (small in metric $\theta$), we
  have:
  \cgather{
    Pr(y \rightarrow x ) \approx Pr(x \rightarrow x)
  }
  which reveals an important special structure on local neighborhoods.
\end{rem}

\section{\vrts Analysis}

\begin{defn}[Algorithm \vrts Parameters]\label{defvrts}
  We introduce three parameters referred tro as teh complexity, surprise and score parameters ($\kappa,\nu,\mu$ respectively) for a given response vector $x$:
  \calign{
    \textrm{complexity: } \ \kappa &\triangleq - \frac{1}{\abs{x}} \ln Pr(x \rightarrow x \vert M^+) = - \frac{\ln \mem{x}^{M^+}}{\abs{x}} \\
    \textrm{surprise: } \   \nu &\triangleq \mathbf{E}_i \left (  1 - \Phi_i^{M^+} (x_{-i}) \vert_{x_i}   \right ) \\
    \textrm{score: } \   \mu &\triangleq \frac{\ln Pr(x \rightarrow x \vert M^+)}{\ln  Pr(x \rightarrow x \vert M^0)} = \frac{\ln \mem{x}^{M^+}}{\ln \mem{x}^{M^0}}
  }
  where $M^+$ indicates the sub-population exhibiting  a particular trait of interest $e.g.$ a mental health disorder such as PTSD, and $M^0$ is the control sub-population where this trait is absent.
\end{defn}

\begin{defn}[Malingering property]
  A response vector $x$  is defined to  have the malingering property if:
  \cgather{
   \chi(x) \triangleq  \big (  \mu(x)  \geqq \mu_0 \big ) \bigwedge \bigg ( \big  (\kappa(x) \leqq \kappa_0\big ) \vee \big (\nu(x) \geqq \nu_0\big )   \bigg ) 
  } %Set of malingering responses is denoted as $\mathscr{M}$. 
\end{defn}
The decision thresholds $\kappa_0,\nu_0,\mu_0$ are  inferred  from  survey data.
\begin{lem}[Complexity]\label{thm-complexity}
  For a survey with $n$ items, and assuming $L$ to the  number of possible responses to each item, the unconditional probability of a response vector $x$ occurring among all feasible responses is bounded above by
  $(e^\kappa / L)^n$, where $\kappa(x) $ is the complexity parameter for response $\x$.
\end{lem}

\begin{proof} Let $\kappa(x) \leqq \kappa'$. From Def.~\ref{defvrts},   we have for a response vector $x$, 
  \cgather{
    -\frac{1}{n} \ln \mem{x} \leqq \kappa' \Rightarrow  \mem{x} \geqq e^{ -n \kappa' }
  }
  Summing on both sides over all responses $x $ with $\kappa(x)\leqq \kappa'$ (assume there are $N_x$ such sequences), we have:
  \cgather{
    1 \geqq \sum_x \mem{x} \geqq \sum_x e^{ -n \kappa' }
  }
  where the first inequality follows from observing that responses very close to $x$ in the \qdist metric have a specific structure, namely $\mem{x} \approx Pr(y \rightarrow x)$ (See Remark~\ref{rem:neighborhood}) and responses further away have smaller jump probabilities, which then  implies:
  \cgather{
    N_x \sum_x e^{ -n \kappa' } \leqq 1 \Rightarrow N_x \leqq e^{ n \kappa' } 
  }
  The result then follows from noting that the complete set of possible responses has the size $L^n$.
\end{proof}
% 
Lemma~\ref{thm-complexity} justifies why a low value of $\kappa$ implies the possibility of an un-natural response, because the odds of generating such a response is remarkably small.
%
\begin{cor}[Algorithmic Complexity]
, The algorithmic complexity of a response $x$ conditional on the number of survey items $n$ is at most  $\kappa(x) + O(1) $.
\end{cor}
\begin{proof}
This follows from noting that a set of cardinality $L^m$ has a algorithmic complexity of $m + O(1)$, since words of length $m$ are sufficient to encode the index of any element of the set, and thus can be uniquely identified. Sinec we can calculate $\kappa'=\kappa(x)$ for any $x$, and since the set of all $x$ for a given value of $\kappa'$ belongs to a set of size at most $e^{ -n \kappa' }$, the result follows.
\end{proof}

\begin{lem}[Surprise]\label{thm-xtalk}
  For any response vector $x$, we have:
  \cgather{ \nu(x) \leqq 1 - e^{-\kappa(x)}}
\end{lem}
% 
\begin{proof}
  Denoting $\Phi_i (x_{-i}) \vert_{x_i}$ as $a_i$, we note that $\mem{x}^{1/n} $ is the geometric mean of the vector of $a_i$s, while $\mathbf{E}_i \left (   \Phi_i (x_{-i}) \vert_{x_i}   \right )$ is the arithmetic mean of the same vector, which then completes the proof by noting:
  \cgather{
    -\mathbf{E}_i \left (   \Phi_i (x_{-i}) \vert_{x_i}   \right ) \leqq -\mem{x}^{1/n} \Rightarrow 
    \nu(x) \leqq 1 - \mem{x}^{1/n} 
  }
\end{proof}
% 
\subsection*{Interpretation on Why the Defined Property Identifies Malingering}

Lemma~\ref{thm-xtalk} indicates that the requirement of  an upper bound on the surprise  and a lower bound on the complexity  are  both aiming to flag responses which are   unlikely to appear when the data (responses) are being generated by the  underlying process  corresponding to the phenotype of interest (PTSD).   When such unlikely responses do appear appear nevertheless, it is likely that they  are not being generated by the correct underlying process.  One can attempt to fake responses that might seem to increase the odds of a positive diagnosis, but the respondant must replicate the cross-dependencies closely enough (build in enough structure) so that the deviation from the expected responses is limitd (limited surprise requirment). But building in too much structure will reduce the complexity too much (too much structure reduces complexity, since there are fewer highly structured sequences), which will then fail the complexity lower bound.

Note that the remaining condition $\mu(x) \geqq \mu_0$ is a diagnosis criterion for the trait of interest ($M^+$), and may be replaced with a different condition if available for identifying participants with  the $M^+$ trait. This particular form follows from a straightforward Bayesian argument on estimating the posterior.

\subsection*{SHAP Analysis Selection of Item Bank}
We present a random subset of $r=20$ items from a master subset of items used in CAT-PTSD. The items that are selected to make this master subset is obtained by ranking the items according to their estimated impact on model pprediction. One standard approach to estimate impact of features on model outcomes is SHAP  (SHapley Additive exPlanations) analysis~\cite{lundberg2017unified}, which is a method derived from game theory to explain the output of machine learning models. It provides a way to measure the contribution of each feature in a given model to the prediction for each instance. In our scenario, we use the persistence function as the model prediction to compute SHAP values, $i.e.$, we rank the items based on the degree to which including an item within a subset of  items with non-empty responses moves the value of the persistence function.

\section{Proof of Theorem~\ref{thm:probbnd}}\label{secproof}


\begin{thm}[Probability bound]\label{thmbnd}
  Given a sequence  $x$ of length $N$ that transitions  to a strain $y\in Q$, we have the following bounds at significance level $\alpha$.
  \cgather{
    \mem{y}^Q e^{ \frac{\sqrt{8}N^2}{1-\alpha}\theta(x,y)} \geqq Pr(x \rightarrow y) \geqq \mem{y}^Q e^{-\frac{\sqrt{8}N^2}{1-\alpha}\theta(x,y)}
  }%
  where $\mem{y}^Q$ is the membership probability of strain $y$ in the target population $Q$ (See Def.~\ref{defmem}), and $\theta(x,y)$ is the q-distance between $x,y$ (See Def.~\ref{defqdistance}).
\end{thm}
\begin{proof}
  Using Sanov's theorem~\cite{cover} on large deviations, we conclude that the probability of spontaneous jump from strain $x\in P$ to strain $y\in Q$, with the possibility $P \neq Q$, is given by:
  \cgather{\label{eq29}
    Pr(x\rightarrow y) =\prod_{i=1}^N \left ( \Phi^P_i(x_{-i}) \vert_{y_i} \right )
  }
  Writing the factors on the right hand side as:
  \cgather{
    \Phi^P_i(x_{-i}) \vert_{y_i} =  \Phi^Q_i(y_{-i}) \vert_{y_i} \left (  \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
  }%
  we note that $\Phi^P_i(x_{-i})$, $\Phi^Q_i(y_{-i})$ are distributions on the same index $i$, and hence:
  \cgather{
    \vert  \Phi^P_i(x_{-i})_{y_i} - \Phi^Q_i(y_{-i})_{y_i}\vert \leqq \sum_{y_i \in \Sigma_i} \vert  \Phi^P_i(x_{-i})_{y_i} - \Phi^Q_i(y_{-i})_{y_i}\vert 
  }%
  Using a standard refinement of Pinsker's inequality~\cite{fedotov2003refinements}, and the relationship of Jensen-Shannon divergence with  total variation, we get:
  \cgather{
    \theta_i \geqq \frac{1}{8} \vert  \Phi^P_i(x_{-i})_{y_i} - \Phi^Q_i(y_{-i})_{y_i}\vert^2
    \Rightarrow \left   \lvert  1  - \frac{\Phi^Q_i(y_{-i})_{y_i}}{\Phi^P_i(x_{-i})_{y_i}} \right \rvert \leqq \frac{1}{a_0}\sqrt{8 \theta_i}
  }%
  where $a_0$ is the smallest non-zero probability value of generating the entry at any index. We will see that this parameter is related to statistical significance of our bounds. First, we can formulate a lower bound as follows:
  \cgather{\label{eqLB}
    \log \left  ( \prod_{i=1}^N   \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
    = \sum_i \log  \left  (  \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
    \geqq \sum_i \left  ( 1- \frac{\Phi^Q_i(y_{-i})_{y_i}}{\Phi^P_i(x_{-i})_{y_i}} \right ) \geqq  \frac{\sqrt{8}}{a_0}\sum_i\theta_i^{1/2} = -\frac{\sqrt{8}N}{a_0}\theta
  }%
  Similarly,  the upper bound may be derived as:
  \cgather{\label{eqUB}
    \log \left  ( \prod_{i=1}^N   \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
    = \sum_i \log  \left  (  \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right ) \leqq \sum_i \left  ( \frac{\Phi^Q_i(y_{-i})_{y_i}}{\Phi^P_i(x_{-i})_{y_i}} - 1 \right ) \leqq \frac{\sqrt{8}N}{a_0}\theta
  }%
  Combining Eqs.~\ref{eqLB} and \ref{eqUB}, we conclude:
  \cgather{
    \mem{y}^Q e^{ \frac{\sqrt{8}N}{a_0}\theta} \geqq Pr(x \rightarrow y) \geqq \mem{y}^Q e^{-\frac{\sqrt{8}N}{a_0}\theta}
  }%
  Now, interpreting $a_0$ as the probability of generating an unlikely event below our desired threshold ($i.e.$ a ``failure''), we note that the probability of generating at least one such event is given by $1-(1-a_0)^N$. Hence if $\alpha$ is the pre-specified significance level, we have for $N >> 1 $:
  \cgather{
    a_0 \approx (1 -\alpha)/N
  }%
  Hence, we conclude, that at significance level $\geqq \alpha$, we have the bounds:
  \cgather{
    \mem{y}^Q e^{ \frac{\sqrt{8}N^2}{1-\alpha}\theta} \geqq Pr(x \rightarrow y) \geqq \mem{y}^Q e^{-\frac{\sqrt{8}N^2}{1-\alpha}\theta}
  }%
\end{proof}

\input{Tables/methods_summary}

\bibliographystyle{naturemag}
\bibliography{qbiome,allbib,tnet}  

\section*{Data, Materials and Software Availability}
Software for inferring \qnet{s} is available as an open-source python package \texttt{quasinet}, and can be
installed from the standard Python code registry.

 
\section*{Acknowledgements}
We extend our appreciation to the PS cohort comprising mental health professionals from around Chicago for their uncompensated participation, and the Prolific survey participants, who received nominal compensation for their invaluable contributions to this study.

\subsection*{Funding}
 This work is funded in part by the Defense Sciences Office of the Defense Advanced Research Projects Agency (Project No. W911NF2010302). The claims made in this study  do not necessarily reflect the position or the
policy of the sponsors, and no official endorsement should be inferred.

\subsection*{Author Contributions}
IC originated  the idea, performed analysis, provided funding, developed software and wrote the paper. NC performed analysis, wrote software and and wrote the paper. RG, RL and JE interpreted data and wrote the paper. 

\subsection*{Regulatory Approvals}
Data collection for the PL and PS cohorts were approved by University of Chicago IRB \#IRB24-0310. The third party platform  Prolific  adheres to rigorous ethical guidelines and privacy policies, ensuring a diverse and reliable participant pool verified through bank-grade ID checks and continuous quality management, aligning with WCAG 2.1 AA standards for accessibility and inclusivity.


\end{document}



