\documentclass[onecolumn,10pt]{IEEEtran}
\let\labelindent\relax
\usepackage{enumitem}
\input{preamble.tex}
\usepackage{float}
% \usetikzlibrary{decorations.pathreplacing,calligraphy}
\input{customdef}
\renewcommand{\baselinestretch}{.97}
% \externaldocument[SI-]{SI}
% \externaldocument[main-]{main}
% \externaldocument[EXT-]{exfig}
\newif\iftikzX
\tikzXtrue
\tikzXfalse 
\newif\ifFIGS  
\FIGSfalse  
\FIGStrue
\cfoot{\scriptsize\thepage}
\lfoot{}
\rfoot{} 
\def\EXTENDEDDATA{Extended Data\xspace}
\def\SUPPLEMENTARY{Supplementary\xspace}
\def\Methods{Online Methods}
\def\SPREFIX{S-}
\def\SPREFIX{}
\def\SUPPLEMENTARY{Extended Data\xspace}

\pgfplotsset{compat=1.16}

\def\Methods{Methods\xspace}

\def\vrts{\texttt{VeRITAS}\xspace}

\begin{document}
\maketitle


{\bf \sffamily \fontsize{10}{12}\selectfont \noindent   
  {\normalfont \itshape Abstract:} Malingering~\cite{Rogers1997,rogers2008clinical} or faking  the symptoms of a mental health disorder  can confound structured diagnostic interviews and hinder clinical psychiatric assessments.  We introduce an  artificial intelligence (AI) framework for detecting symptom fabrication in Post-Traumatic Stress Disorder (PTSD) diagnoses, for which malingering is a known problem~\cite{frueh2007us,taylor2007detection}, partially ascribable to the  potential for secondary financial gain from positive diagnoses.  Algorithm \vrts employs novel generative AI to infer statistical dependencies inherent in true response patterns, and flags  responses which violate these subtle  constraints. Mimicing these  emergent  patterns is difficult  even  with psychiatric training, resulting in a robust mechanism for recognizing fabrications. With a study sample of $n=624$ patients, \vrts is estimated to have a   Area Under the Curve (AUC) of $\geqq 95.0\%$, with one sensitivity/specificity pair  of 92\% and 95\% respectively. Our tool   offers an  objective, disease-specific, fast (average time $\leqq 2$ min)  approach to detect fake  PTSD, and if adopted, can  ensure that healthcare resources and disability concessions reach  those genuinely in need,  while  helping to maintain  integrity of clinical data. Moreover, the ability to identify and help  patients who might be malingering due to other mental health conditions, poverty or   socio-economic compulsions can  improve general health outcomes  in disadvantaged communities.  
}







\vspace{10pt} 

\section*{Introduction}
Diagnosis of mental health disorders typically  rely  on structured interviews~\cite{ali2015multimodal} susceptible to  intentional fabrication of  symptoms~\cite{Rogers1997,rogers2008clinical}, often referred to as ``malingering''. In the specific context of clinically diagnosing Post-Traumatic Stress Disorder (PTSD),  subjective stressors, a high degree of similarity in presentation,  the realtively easy access to  information on  how to fake PTSD,  and financial  incentives from a positive diagnosis  in  legal procedures or   disability claims, is known to hinder   accurate   assessments. Here, we present an approach based on a  generative model to flag malingering suspects rapidly, efficiently and accurately; essentially demonstrating a generative artificial intelligence (AI) framework that, paraphrasing Ekman~\cite{Ekman1992},  ``\textit{can catch a liar}''. 

Clinically, PTSD is an anxiety disorder that can develop after experiencing a traumatic event. In the United States,  substantial disability compensation may be  available for  those with mental disorders, which while being an crucial resource for the truely afflicted, incentivizes malingering~\cite{frueh2007us,taylor2007detection}.  Thus,  faking PTSD to access medical treatment, commit insurance, personal injury and other frauds,  or in an attempt to evade criminal liability and penalties~\cite{guriel2003assessing,salloway1990opiate,resnick2008malingering,burkett1998stolen} is unfortunately not rare. While  PTSD is a serious mental health condition  associated with  substance use disorder, mood disorder, anxiety disorder,  personality disorder, increased morbidity,  and possibly with increased  mortality~\cite{goldstein2016epidemiology,schnurr2009posttraumatic}, false  diagnoses can cause substantial financial drain~\cite{lopiccolo1999current, oboler2000disability} to  healthcare systems, divert crucial resources from where they are needed~\cite{taylor2006clinician}, and interfere with study outcomes by introducing inaccuracies in clinical data~\cite{rosen2006dsm}. Accurate disambiguation of true and fake PTSD is therefore  of high importance, especially with sources suggesting that over 20\% of  personal injury cases, as well as  20\%  of the Veterans seeking combat compensation could be  fabricating their condition~\cite{marx2011ptsd,rogers1994explanatory,leeshaley1997mmpi2,frueh2007us}.
In addition to curbing  financial fraud, ability to flag such incongruities can help address the myriad of   factors that can  drive malingering behavior, including other mental health conditions, a lack of access to healthcare~\cite{park2021race,muntaner2004socioeconomic} and  an inability to seek help arising from poverty and other broad-ranging socio-economic conditions.

\begin{figure}
  \tikzexternalenable
  \tikzsetnextfilename{scheme}
  \centering
  % \tikzXtrue
  \iftikzX  
  \input{Figures/scheme}
  \else
  \includegraphics[width=.92\textwidth]{Figures/External/scheme}
  \fi
 
  \vspace{-12pt}
  
  \captionN{\textbf{Conceptual framework.} Using a  dataset of responses to a validated structured interview for PTSD diagnoses, along with physician-validated clinical diagnoses, we infer generative models for responses for PTSD patients. In our framework for detecting  malingering, we flag responses as those which are highly ``surprising'' (defined as violating infered cross-dependencies between  individual response items) or are too simple (lacking complexity in the response patterns typical of non-malingered responses). The precise ``malingering condition'' shown above is validated from theoretical considerations as well as field experimental data.}\label{qnet}
\end{figure}

Despite the general difficulty in formulating principles to detect malingering~\cite{drob2009clinical}, multiple standardized tests~\cite{Wong2005,smith1997detection} and  validity assessment tools~\cite{ben2012interpreting} have been proposed, with limited success. These tools typically aim to incorporate patterns observed in diagnostic populations that might disambiguate faked symptoms from real ones, or ask similar or related questions multiple times to verify consistency. However,  existing approaches  do not target specific disorders, almost always require expert interpretation, are  subjective, and by design are  unlikely to be effective against a malingerer with psychiatric training (See Table~\ref{tbl:methods_summary}). Other  strategies with physiological monitoring and linguistic analysis~\cite{ekman1991who,mihalcea2009lie,burgoon2008cognitive,zhou2004automating}  cannot be easily  adopted  in structured or semi-structured interviews.

In  \vrts we leverage the fact that
responses  in a structured interview have  statistical dependencies  arising from the nature of the questions themselves, and are modulated by the trait we are aiming to detect $e.g.$, PTSD pathology. We operationalize this principle without requiring human-understanding of the specific items being presented to the subject; thus making the approach   specific to the disease at hand (PTSD), while being  potentially generalizable to other disorders if appropriate training data is available.

Our key finding here is that the subtle cross-dependencies between the interview items   are challenging to  mimic on-the-fly, even if the subject is  knowledgeable about how such patients tend to respond $i.e.$ with training in the mental health services.   Thus,  \vrts offers a robust  approach to verifying response validity, can target specific disorders,  can be administered in less that $4$ minutes, and require little subjective interpretation.

% (3.851126325864842, 2.879873674135158) min at 95% confidence
\def\Mpos{M^+}
\def\Mcont{M^0}

\section*{Results} 

% and provided responses on the PTSD Checklist (PCL-5) for Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5)



\subsection*{Participants and Data Sources}
Our  first   dataset  (referred to as the VA dataset)  comprises  $n=304$ participants  recruited at a Veterans Health Administration facility for an earlier study~\cite{brenner2021development}. Veterans  between the ages of 18 and 89 years were recruited with written informed consent. Once eligibility was determined by the study team, participants completed a PTSD-symptom questionnaire from the CAT-PTSD item bank~\cite{gibbons2012development,brenner2021development}, comprising $211$ items, including some items from the  PTSD Checklist (PCL-5). Participants  were also interviewed using the Clinician-Administered Scale for PTSD for Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), which resulted in $86$ PTSD  diagnoses, and $218$ participants  deemed as  not having PTSD. We used {\color{Red1} 60\%} of the data for inferring our generative models, and the rest was used for validation, including determining the out-of-sample AUC for identifying PTSD vs no-PTSD cases. The possibility of malingering was not recorded in this dataset.

The second dataset generated in the course of the present study is from online surveys conducted by a third party vendor (Prolific) hired by the study team. Referred to here as the Prolific dataset, it comprises $n= 310$ participants (200 in the US, and 110 in the UK), screened for the absence of past or present mental health diagnoses. The participants were asked to fake symptoms of PTSD while taking the \vrts interview. 

In total, this study considered {\color{Red1} $n=624$} participants comprising  US Veterans, and drawn from the  general population in the US and the UK, with broad representation across sexes and ethnicities. Average completion times and detailed demographic composition of the respondents is shown in Table~\ref{tabprolific}.


\begin{table}[t]
  \centering
  
  \captionN{Prolific Dataset Participant Characteristics and Success Rate as a percentage of the representation, at 90\% sensitivity}\label{tabprolific}
\input{Figures/tabprolific}
\end{table}

\subsection*{Item Presentation and Response Sequence in Administration}
In \vrts we present  a small random subset of the full item bank to the subjects. Although the models for cross-dependency are inferred using the complete response sequence to all 211 items available, presenting a smaller  item set makes the test feasible, while also producing a vast number of possible variations. In the version we validated, 20 items are presented. These are randomly generated from a distribution reflecting the  impact  of the items in the inferred models on their ability to disambiguate between responses from diagnosed PTSD patients and the control group. Impact is measured using standard SHAP analysis of the inferred models (See \Methods for details). SHAP-values for items are normalized to a probability mass function (pmf), which is then sampled  to produce the item set. We can select items adaptively similar to CAT-PTSD~\cite{brenner2021development}; such  complications have been presently avoided.


\subsection*{Integrated Diagnostic Capability}
Given a sequence of responses to a diagnostic interview, our first task is to determine if a particular subject should be diagnosed with PTSD,  if the  possibility of  malingering is ignored. We call this a ``naive diagnosis''. This diagnostic information might be available to \vrts externally ($e.g.$ from a physician's assessment of the patients). However, \vrts also has an integrated capability for naive diagnosis: we identify separate  generative models for 1) the diagnosed set of patients in the training set ($\Mpos$, some of them might be malingering) and 2) for the patients identified as not having PTSD ($\Mcont$). Then given the sequence of responses from a new subject, we estimate if that sequence is more likely to have been generated by the model for  $\Mpos$ vs that for $\Mcont$. We validate this diagnostic capability  using the training data described above, and we achieve good disambiguation between $\Mpos$ and $\Mcont$, with out-of-sample AUC  {\color{Red1}$0.867\pm 0.008$}, which is at par or better compared to reported tools, $e.g.$,  CAT-PTSD~\cite{Brenner_2021}. In the \vrts algorithm, for a given response sequence $\x$, the naive diagnosis risk score is denoted as $\mu(x)$, and referred to as simply the ``score'' (See \Methods for definition and computational approach).

%model 9

\subsection*{Principle of Characterizing Malingered Responses}
Amongst those who have a naive PTSD diagnosis, we aim to flag subjects with  a quantifiable high estimated likelihood of malingering. Note that the in training set we only have the naive diagnoses information; thus some participants diagnosed with PTSD in the dataset could have been malingering, but are not flagged to be so. To flag malingering sunjects, our first insight  is that such response sequences  have high average ``surprise'', $i.e. $ deviate more on average from the context-specific model predictions of item responses. Our other insight is that malingering tend to generate  less complex patterns in the response sequences. We understand complexity in the formal sense: more complex objects are less compressible, and  random sequences are not significantly compressible. Thus a sequence of all 1's is very compressible and therefore non-random, but a sequence generated from teh sequential tosses of a fair coin is not very compressible, and is highly random. We hypothesize that true response sequences  will tend to have maximal randomness, conditional on  being constrained by the emergent cross-dependencies. Thus, true responses  should have the just right  degree of identifiable structure, and  no more. More structure will make it less random, and less structure will increase surprise, and thus too much and too little structure are both indicative of malingering. Both of these criteria are based on statistical properties of the response sequence, and do not require a human understanding of the items. Quantitatively, for each response sequence $\x$, we compute two quantities: $\kappa(x)$ and  $\nu(x)$, referred to as the complexity  and the surprise parameters respectively.

Thus,  $\kappa, \nu, \mu$ are  random variables which are functions of the response sequence of the subject, and have distributions that may be characterized from the VA data, despite  not having designation of ``malingering'' in that  dataset. This is because that these quantities are computable from just the response sequence themselves. Concretely, we propose that a subject with a response sequence $\x$  is malingering if for a suitably chosen thresholds $\mu_0,\nu_0, \kappa_0$:
 \cgather{
    \big (  \mu(x)  \geqq \mu_0 \big ) \bigwedge \bigg ( \big  (\kappa(x) \leqq \kappa_0\big ) \vee \big (\nu(x) \geqq \nu_0\big )   \bigg ) 
  }
This may be paraphrased as that  a response sequence has high likelihood of malingering if it 1)  produces a naive diagnosis of PTSD with high probability, and 2) is either too surprising or too simple. The decision thresholds are obtained from theoretical considerations and the VA data.

\subsection*{\vrts Validation Strategy}
Since the VA data does not not direct indication of either presence of absence of malingering, we adopted an non-standard approach for validating \vrts. We assumed that the Prolific participants do not have PTSD, and that all of them were attempting to malinger. This allows us to directly measure false negative rates, as function of the \vrts parameters. Then we check how many of the VA participants are flagged as malingering amongst the ones with a naive PTSD diagnosis. To determine a lower bound on performance we can assume all of these subjects are false positives (which is unlikely, but nevertheless is an upper bound on false positives as function of \vrts parameters). This allows us to construct a lower envelop of the ROC curve, and hence estimate a lower bound of the AUC.

Using this approach, we obtain a minimum AUC of 95\%.

  

\subsection*{\qnet Inference and Cross-talk Modeling}


\subsection*{Training and Validation}

\subsection*{Performance}

\subsection*{Comparison Against State of Art}



% \begin{figure}
%   \includegraphics[width=\textwidth]{Figures/fig2}
%   \captionN{}\label{fig2}
% \end{figure} 


\section*{Discussion}
Malingering remains a persistent problem, especially in psychiatric and criminal justice settings, with prevalence estimates ranging from $8-64\%$\cite{mcdermott2013malingering,schmidt2020base,matto2019systematic}. Here we develop a novel system to address this gap for  the need for further development of reliable, rigorous, and principled methods for detecting deception remains highly significant~\cite{DePaulo2003}.

{\color{Red1} In this study, we introduce a novel algorithmic technique for identifying  deception in structured interviews. This approach first infers a detailed model of the response patterns of previous respondents, then uses a measure of consistency known as dissonance to identify if future respondents' answers are indicative of deception.}

Our model-based, data-driven approach is notably distinct from traditional methods of malingering detection, which are typically constructed for  specific contexts~\cite{walczyk2018review}. in the context of  psychiatric diagnosis detection of faked symptoms is often based on standardized tests; some commonly used ones are summarized in Table \ref{tbl:methods_summary}. Tests such as the Structures Interview of Reported Symptoms (SIRS)~\cite{Wong2005}, Structured Inventory of Malingered Symptomology (SIMS)~\cite{smith1997detection}, and the Minnesota Multiphasic Personality Inventory-2 and its associated validity scales~\cite{ben2012interpreting} draw extensively on research of both legitimate psychiatric patients and malingerers, and employ a combination of detection strategies focused on symptoms that are either unlikely to be presented by genuine patients, or that tend to be amplified in malingerers~\cite{rogers2008determinations}. For detection of cognitive malingering, physiological, arousal-based approaches such as the Control Question Technique\cite{vrij2001killed}, and cognitive load-inducing approaches such as requiring subjects to perform a concurrent task ($e.g.$ maintaining eye contact) and the Autobiographical Implicit Association Test (aIAT) have been suggested.

Accuracy rates for these research-driven state of the art detection techniques are typically in the range of $85$-$95$\% depending on the specific problem context\cite{sartori2008accurately,gregg2007vying,monaro2017detection} (similarly high sensitivities have also been reported \cite{smith1997detection}). However, among the limitations to these methods is the requirement of substantial domain research and/or subject matter expertise to develop. Thus while these methods perform well in the specific context for which they were developed, it may be challenging and/or expensive to broadly extend to detection in other contexts. In the case of cognitive malingering methods, vulnerabilities to clinician/adminstrator bias, high rates of false positives and false negatives, and vulnerability to coaching have been noted as possible limitations. 

In contrast with these methods, our results demonstrate that it is possible to achieve similar (or perhaps improved) performance in detecting malingering by identifying structural differences in an individual's responses compared to a set of baseline responses. The basic insight underpinning our results is that consistent, genuine responses to interview questions are typically not independent. In general, there are dependence patterns between interview questions, which are often nontrivial and difficult or impossible to identify a priori. Given the entirety of responses an individual has provided in an ongoing interview, the response probabilities for a consistent response to the next question are governed by these dependence patterns. As these probabilistic constraints exist between all questions in the interview, a complex interaction network is induced over the high-dimensional response space. To infer the structure of this network, we use a novel architecture - the \qnet, a nonparametric generative model of the response network which reflects the $n$-way inter-question relations found amongst previous interview respondents.

From the model's approximation of the probabilistic dependencies present in the response network, we derive the dissonance as an intuitive, individually computable measure of response consistency. For all data sets in this study, mean dissonance values for random malingering and expert malingering are significantly higher than for actual responses, empirically demonstrating its utility for identifying such responses. As should be expected, we also found that the expert malingering responses were on average less dissonant than random malingering, showing that domain knowledge does allow individuals to respond in a more convincing manner. However, their failure to adequately account for detailed inter-question dependencies tends to make their responses significantly different from actual responses in a measurable, quantitative sense. 

The classification results obtained further demonstrate the ability of this framework to identify subtle deviations in the response patterns of actual and malingering respondents. Due to the lower dissonance generated by expert malingerers, we found it more difficult to identify such responses; however, in practice it appears unlikely that even the most knowledgeable domain experts would have the requisite quantitative knowledge of response distributions of the target phenotype to be able to respond in the manner of our idealized expert malingerers. Thus in real-world usage where true malingerers are likely a mixture of these two idealizations, we would expect performance to be bounded above and below by the empirical performance obtained for the random and expert scenarios.

In general, data-driven methods for detection of malingering are not as ubiquitous as in other domains; however, the increasing availability of modern sensory data collection has allowed for implementation of machine learning algorithms and computer vision techniques~\cite{Chan2018, Wang2020, Lee2019} for detecting deception based on patterns in language, facial expressions, body movements, and eye movements~\cite{Yu2018, Ducharme2017}. These methods have been shown to outperform traditional lie detection methods in some cases, obtaining accuracy rates of over 90\% for lie detection from patterns in facial expressions and body movements~\cite{Ekman1992,Yu2018} and 60-80\% using analysis of gaze direction and pupil dilation~\cite{Ducharme2017}%\cite{Granziero2019}.
. However, in many instances it is not feasible to collect detailed sensory data, possibly limiting the general applicability of these kinds of methods. By exclusively utilizing response data, our dissonance-based approach is not subject to such limitations.

% The basis of our algorithmic approach is a detailed model of the complex probabilistic dependencies present between an individual's responses to an interview question. Given that a response to a question is truthful, probabilities of subsequent responses to other related questions are changed. A priori, such dependencies are present amongst all possible questions in a given interview, which yields a combinatorial explosion of interactions to consider. For the PTSD data set, we have $211$ questions, which means that each individual's responses may be identified with a point in a $211$-dimensional space where the response to each question in principle depends on the other $210$ questions.

In contexts such as these, where malingerers seek to obtain a target diagnosis, training data from past respondents often includes a clinical phenotype for each respondent. However, this is not always available (as was observed for the CCHHS data set). Nonetheless, in the absence of the underlying diagnostic phenotypes, our approach still appears to perform well under a scenario in which expert malingerers are also unsure of ground-truth diagnoses. 

In other types of structured interviews, such as internet surveys and opinion polls, built-in data validity checks are frequently used to attempt to proactively identify disingenuous respondents. Among such checks, unusually fast response times have been observed to be indicative of poor data quality\cite{malhotra2008completion,callegaro2009response}, demonstrating a lack of attention by interview respondents\cite{greszki2015exploring}. Our finding of a negative correlation between dissonance and response time adds some support to this notion. However, it has been noted that duration-based validity checks may be vulnerable to a high false negative rate\cite{kennedy2020assessing}, suggesting a need for most robust measures of assessing validity. 

\section*{Methods}

\section{Definitions \& Notation}

\begin{defn}[Survey]
  A survey for the purpose of this work is a structured interview, consisting of a finite number of questions (items) posed to a set of participants, with these items drawn from a finite item bank, and  whose responses must be one froma pre-specified set fo choices, $e.g.$, the Likert scale, with missing values for the responses allowed. 
\end{defn}


\begin{defn}[Response vector]
  A response vector is the set of responses to a survey from a single participant, typically assuming that not all items are posed, and allows for the possibility that some responses are missing.
\end{defn}


A \qnet, as described here, is a model of the response dependency  structure for questions (items) posed to participants in a survey.  The \qnet explicitly estimates individual conditional distributions of each item response, which collectively serve as a model of the full joint distribution of the responses. 

\begin{defn}[\qnet]
  \label{def:qnet}
  Let $X \sim P$ be an $n$-dimensional discrete random vector supported on a finite set $\Sigma$ and following distribution $P$, i.e. \[X = (X_1, \ldots, X_n) \sim P, \hspace{0.25in} \operatorname{supp}(X) =  \Sigma = \displaystyle\prod_{i=1}^n \Sigma_i \hspace{.1in} \text{with } |\Sigma| < \infty.\] For $i = 1, \ldots, n$, let $P_i := P(X_i\,|\,X_j=x_j \text{ for } j \neq i)$ denote the conditional distribution of $X_i$ given the values of the other components of $X$.  Finally, for each $i = 1, \ldots, n$, let $\Phi^P_i$ denote an estimate of the distribution $P_i$.  Then the set $\Phi^P := \{\Phi^P_i\}_{i=1}^n$ is called a \emph{Quasinet (\qnet)}. Identifying the true distribution $P$ as the one describing the joint statistics of the responses from a survey with $n$ items, we also refer to  $\Phi^P$ as the \qnet for the survey $P$.
\end{defn}

When $P$ is clear from context, we may omit the superscript and simply write $\Phi = \{\Phi_i\}$ to denote the \qnet. The motivation for Definition \ref{def:qnet} is that the collection of all estimators $\Phi = \{ \Phi_i \}$ contained in a \qnet represents the set of all inferred dependencies from the observed ecosystem.  While the definition allows for arbitrary method of algorithm to construct the estimators $\Phi_i$, the utility of a \qnet clearly depends primarily on the properties of the $\Phi_i$.  In this study, we aim to minimize the set of a priori assumptions on the overall model structure to allow the complex dependencies present in $P$ to emerge. To that end, throughout this work all {\qnet}s are computed using conditional inference trees~\cite{sarda2017conditional} (a variant of classification and regression trees) to compute each $\Phi_i$. In general each \qnet component $\Phi_i$ is computed independently from the other $\Phi_j$, which allows a network structure to emerge amongst these estimators.

An important quantity for an inferred \qnet is the persistence function $\mem{\x}$.

% 
\begin{defn}[Persistence Function]\label{def:mem}
  Given a survey $P$ inducing the \qnet $\Phi^P$ and a response  vector $\x = (x_1, \ldots, x_n)$, the persistence $\mem{\x}$ of  $\x$ in the population modeled  by the \qnet:
  \cgather{
    \mem{\x}^P := \operatorname{Pr}(\x \in P) = \prod_{i=1}^n  \Phi^P_i(X_i = x_i\,|\,X_j = x_j, j \neq i) %
  }%
\end{defn}
The persistence function $\mem{\x}^P$, as the name suggests,  is the probability that $\x$ persists, $i.e.$,  $Pr(\x \rightarrow \x)$ for the population modeled by the \qnet $P$, with $1-\mem{\x}^P$ being the probability that $\x$ is altered by a  random perturbation.

We will show  that if for two inferred \qnet models $P,Q$, we have $\mem{\x}^P \geqq \mem{\x}^Q$, then it is more likely that model $P$ generated $\x$. This is an important result that justifies the definition of the
score parameter in Defn.~\ref{defvrts}.

% \begin{wrapfigure}[16]{r}{1.7in}
%   \centering

%   \vspace{-17pt}

%   \includegraphics[width=1.65in]{Figures/responsecartoon.pdf}
%   \captionN{Why response vectors might change spontaneously: if  response to an item is a   sample from a  distribution over the possible responses, two responses with similar probabilities  can switch easily, thus making a spontaneous perturbation possible. }\label{figcartoon}
% \end{wrapfigure}
The \qnet allows us to rigorously compute  bounds on  the probability of a spontaneous change from one response vector to another, induced by spontaneous chance variations (Fig~\ref{figcartoon}). Not all perturbations in a vector are either likely or contextually meaningful. With an exponentially exploding number of possibilities in which a vector over a large set of items can vary, it is computationally intractable to directly model all possible dependecies; nevertheless, we can constrain the possibilities using the patterns we uncover via the \qnet construction.  A key piece of this approach is to design  an intrinsic distance between response vectors, which is reflective of this underlying  dependency structure.  

\begin{defn}[\qdist]
  \label{defqdistance} 
  Let $\Phi^P = \{\Phi_i^P\}_{i=1}^n$ and $\Phi^Q = \{\Phi_i^Q\}_{i=1}^n$ denote {\qnet}s on populations $P$ and $Q$, and suppose $\x=(x_1, \ldots, x_n)$ and $\y =(y_1, \ldots, y_n)$ are samples of $X \sim P$ and $Y \sim Q$ respectively. Then the \qdist $\theta_{P,Q}(\x,\y)$ between $\x$ and $\y$ is\[\theta_{P,Q}(\x,\y) := \frac{1}{n}\sum_{i=1}^n \left[ \mathbb{J}^{\frac{1}{2}} \left(\Phi_i^P(X_i |X_j = x_j, j \neq i) \,\|\, \Phi_i^Q(Y_i|Y_j = y_j, j \neq i\right ) \right]\]%
  where $\mathbb{J}$ denotes the Jensen-Shannon divergence~\cite{cover}.
\end{defn}

For brevity, we may write simply $\theta$ (dropping the suffixes) if the populations are clear from context.  Since the Jensen-Shannon distance $\mathbb{J}$ is a legitimate metric~\cite{fuglede2004jensen} on the set of probability distributions (unlike KL-divergence), $\theta$ inherits nonnegativity, symmetry, and respects the triangle inequality; it follows that \qdist is a (pseudo)-metric on $\Sigma$. Note that, being a pseudo-metric implies that  we may have $\theta(\x, \y) = 0$ for $\x \neq \y$, i.e. distinct vectors can induce the same distributions over each index, and thus have zero distance. This is in fact desirable, since we do  not want  our distance to be sensitive to changes that are not meaningful.  The intuition is that not all variations are equally important or likely.    Moreover, we show in Theorem \ref{thm:probbnd} that the log-likelihood of a vector $\x$ transitioning to $\y$ scales with $\theta(\x,\y)$, allowing us to directly estimate the probability of spontaneous (or sequential) jumps between abundance profiles.

\begin{thm}[Probability Bound]\label{thm:probbnd}
  Given a vector $\x$ of length $n$ from $P$ that transitions to $\y$ from $Q$, we have the following bounds at significance level $\alpha$.
  \cgather{
    \mem{y} e^{ \frac{\sqrt{8}N^2}{1-\alpha}\theta(\x,\y)} \geqq Pr(\x \rightarrow \y) \geqq \mem{\y} e^{-\frac{\sqrt{8}N^2}{1-\alpha}\theta(\x,\y)}
  }%
  where $\mem{\y}$ is the persistence of  $\y$  (Def.~\ref{def:mem}), and $\theta(\x,\y)$ is the q-distance between $\x,\y$ (Def.~\ref{defqdistance}).
\end{thm}

\begin{proof}
  See later in Section~\ref{secproof}.
\end{proof}

Theorem 1 gives theoretical backing to the claim that samples generated by the \qnet indeed reflect likely perturbation possibilities from the current state.  Thus we can use the \qnet to draw contextually realistic samples that respect the  cross dependencies and reduce surprise  (that is, the \qnet-inferred conditional distributions can be used to  generate approximate samples from the population $P$).  This has several implications, such as the ability to easily handle missing/incomplete data. 


\begin{rem}[Neighborhood Structure]\label{rem:neighborhood}
  It follows from Th.~\ref{thm:probbnd} that we have for some constant $C$,
  \cgather{
    \ln \left \vert \frac{Pr(x \rightarrow y)}{Pr( y \rightarrow y)} \right \vert \leqq C \theta(x,y)
  }
  implying for all response vectors $y$ within a small neighborhood of $x$ (small in metric $\theta$), we
  have:
  \cgather{
    Pr(y \rightarrow x ) \approx Pr(x \rightarrow x)
  }
  which reveals an important special structure on local neighborhoods.
\end{rem}

\section{\vrts Analysis}

\begin{defn}[Algorithm \vrts Parameters]\label{defvrts}
  We introduce three parameters referred tro as teh complexity, surprise and score parameters ($\kappa,\nu,\mu$ respectively) for a given response vector $x$:
  \calign{
    \textrm{complexity: } \ \kappa &\triangleq - \frac{1}{\abs{x}} \ln Pr(x \rightarrow x \vert M^+) = - \frac{\ln \mem{x}^{M^+}}{\abs{x}} \\
    \textrm{surprise: } \   \nu &\triangleq \mathbf{E}_i \left (  1 - \Phi_i^{M^+} (x_{-i}) \vert_{x_i}   \right ) \\
    \textrm{score: } \   \mu &\triangleq \frac{\ln Pr(x \rightarrow x \vert M^+)}{\ln  Pr(x \rightarrow x \vert M^0)} = \frac{\ln \mem{x}^{M^+}}{\ln \mem{x}^{M^0}}
  }
  where $M^+$ indicates the sub-population exhibiting  a particular trait of interest $e.g.$ a mental health disorder such as PTSD, and $M^0$ is the control sub-population where this trait is absent.
\end{defn}


\subsection*{Interpretation of the Parameters}
We 



\begin{defn}[Malingering property]
  A response vector $x$  is defined to  have the malingering property if:
  \cgather{
    \big (  \mu(x)  \geqq \mu_0 \big ) \bigwedge \bigg ( \big  (\kappa(x) \leqq \kappa_0\big ) \vee \big (\nu(x) \geqq \nu_0\big )   \bigg ) 
  } Set of malingering responses is denoted as $\mathscr{M}$. 
\end{defn}
The decision thresholds $\kappa_0,\nu_0,\mu_0$ are  inferred  from  survey data.
\begin{lem}[Complexity]\label{thm-complexity}
  For a survey with $n$ items, and assuming $L$ to the  number of possible responses to each item, the unconditional probability of a response vector $x$ occurring among all feasible responses is bounded above by
  $(e^\kappa / L)^n$, where $\kappa(x) $ is the complexity parameter for response $\x$.
\end{lem}

\begin{proof} Let $\kappa(x) \leqq \kappa'$. From Def.~\ref{defvrts},   we have for a response vector $x$, 
  \cgather{
    -\frac{1}{n} \ln \mem{x} \leqq \kappa' \Rightarrow  \mem{x} \geqq e^{ -n \kappa' }
  }
  Summing on both sides over all responses $x $ with $\kappa(x)\leqq \kappa'$ (assume there are $N_x$ such sequences), we have:
  \cgather{
    1 \geqq \sum_x \mem{x} \geqq \sum_x e^{ -n \kappa' }
  }
  where the first inequality follows from observing that responses very close to $x$ in the \qdist metric have a specific structure, namely $\mem{x} \approx Pr(y \rightarrow x)$ (See Remark~\ref{rem:neighborhood}) and responses further away have smaller jump probabilities, which then  implies:
  \cgather{
    N_x \sum_x e^{ -n \kappa' } \leqq 1 \Rightarrow N_x \leqq e^{ n \kappa' } 
  }
  The result then follows from noting that the complete set of possible responses has the size $L^n$.
\end{proof}
% 
Lemma~\ref{thm-complexity} justifies why a low value of $\kappa$ implies the possibility of an un-natural response, because the odds of generating such a response is remarkably small.
% 
\begin{lem}[Surprise]\label{thm-xtalk}
  For any response vector $x$, we have:
  \cgather{ \nu(x) \leqq 1 - e^{-\kappa(x)}}
\end{lem}
% 
\begin{proof}
  Denoting $\Phi_i (x_{-i}) \vert_{x_i}$ as $a_i$, we note that $\mem{x}^{1/n} $ is the geometric mean of the vector of $a_i$s, while $\mathbf{E}_i \left (   \Phi_i (x_{-i}) \vert_{x_i}   \right )$ is the arithmetic mean of the same vector, which then completes the proof by noting:
  \cgather{
    -\mathbf{E}_i \left (   \Phi_i (x_{-i}) \vert_{x_i}   \right ) \leqq -\mem{x}^{1/n} \Rightarrow 
    \nu(x) \leqq 1 - \mem{x}^{1/n} 
  }
\end{proof}
% 
\subsection*{Why the Defined Property Identifies Malingering}

Lemma~\ref{thm-xtalk} indicates that the requirement of  an upper bound on the surprise  and a lower bound on the complexity  are  both aiming to flag responses which are   unlikely to appear when the data (responses) are being generated by the  underlying process  corresponding to the phenotype of interest (PTSD).   When such unlikely responses do appear appear nevertheless, it is likely that they  are not being generated by the correct underlying process.  Can it correspond to another ``natural'' process, $e.g.$ a different mental disorder that is not intentional fabrication? If the score is higher than 




Note that the remaining condition $\mu(x) \geqq \mu_0$ is a diagnosis criterion for the trait of interest ($M^+$), and may be replaced with a different condition if available for identifying participants with  the $M^+$ trait. This particular form follows from a straightforward Bayesian argument on estimating the posterior.

\section{Proof of Theorem~\ref{thm:probbnd}}\label{secproof}


\begin{thm}[Probability bound]\label{thmbnd}
  Given a sequence  $x$ of length $N$ that transitions  to a strain $y\in Q$, we have the following bounds at significance level $\alpha$.
  \cgather{
    \mem{y}^Q e^{ \frac{\sqrt{8}N^2}{1-\alpha}\theta(x,y)} \geqq Pr(x \rightarrow y) \geqq \mem{y}^Q e^{-\frac{\sqrt{8}N^2}{1-\alpha}\theta(x,y)}
  }%
  where $\mem{y}^Q$ is the membership probability of strain $y$ in the target population $Q$ (See Def.~\ref{defmem}), and $\theta(x,y)$ is the q-distance between $x,y$ (See Def.~\ref{defqdistance}).
\end{thm}
\begin{proof}
  Using Sanov's theorem~\cite{cover} on large deviations, we conclude that the probability of spontaneous jump from strain $x\in P$ to strain $y\in Q$, with the possibility $P \neq Q$, is given by:
  \cgather{\label{eq29}
    Pr(x\rightarrow y) =\prod_{i=1}^N \left ( \Phi^P_i(x_{-i}) \vert_{y_i} \right )
  }
  Writing the factors on the right hand side as:
  \cgather{
    \Phi^P_i(x_{-i}) \vert_{y_i} =  \Phi^Q_i(y_{-i}) \vert_{y_i} \left (  \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
  }%
  we note that $\Phi^P_i(x_{-i})$, $\Phi^Q_i(y_{-i})$ are distributions on the same index $i$, and hence:
  \cgather{
    \vert  \Phi^P_i(x_{-i})_{y_i} - \Phi^Q_i(y_{-i})_{y_i}\vert \leqq \sum_{y_i \in \Sigma_i} \vert  \Phi^P_i(x_{-i})_{y_i} - \Phi^Q_i(y_{-i})_{y_i}\vert 
  }%
  Using a standard refinement of Pinsker's inequality~\cite{fedotov2003refinements}, and the relationship of Jensen-Shannon divergence with  total variation, we get:
  \cgather{
    \theta_i \geqq \frac{1}{8} \vert  \Phi^P_i(x_{-i})_{y_i} - \Phi^Q_i(y_{-i})_{y_i}\vert^2
    \Rightarrow \left   \lvert  1  - \frac{\Phi^Q_i(y_{-i})_{y_i}}{\Phi^P_i(x_{-i})_{y_i}} \right \rvert \leqq \frac{1}{a_0}\sqrt{8 \theta_i}
  }%
  where $a_0$ is the smallest non-zero probability value of generating the entry at any index. We will see that this parameter is related to statistical significance of our bounds. First, we can formulate a lower bound as follows:
  \cgather{\label{eqLB}
    \log \left  ( \prod_{i=1}^N   \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
    = \sum_i \log  \left  (  \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
    \geqq \sum_i \left  ( 1- \frac{\Phi^Q_i(y_{-i})_{y_i}}{\Phi^P_i(x_{-i})_{y_i}} \right ) \geqq  \frac{\sqrt{8}}{a_0}\sum_i\theta_i^{1/2} = -\frac{\sqrt{8}N}{a_0}\theta
  }%
  Similarly,  the upper bound may be derived as:
  \cgather{\label{eqUB}
    \log \left  ( \prod_{i=1}^N   \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right )
    = \sum_i \log  \left  (  \frac{\Phi^P_i(x_{-i}) \vert_{y_i}}{\Phi^Q_i(y_{-i}) \vert_{y_i}}  \right ) \leqq \sum_i \left  ( \frac{\Phi^Q_i(y_{-i})_{y_i}}{\Phi^P_i(x_{-i})_{y_i}} - 1 \right ) \leqq \frac{\sqrt{8}N}{a_0}\theta
  }%
  Combining Eqs.~\ref{eqLB} and \ref{eqUB}, we conclude:
  \cgather{
    \mem{y}^Q e^{ \frac{\sqrt{8}N}{a_0}\theta} \geqq Pr(x \rightarrow y) \geqq \mem{y}^Q e^{-\frac{\sqrt{8}N}{a_0}\theta}
  }%
  Now, interpreting $a_0$ as the probability of generating an unlikely event below our desired threshold ($i.e.$ a ``failure''), we note that the probability of generating at least one such event is given by $1-(1-a_0)^N$. Hence if $\alpha$ is the pre-specified significance level, we have for $N >> 1 $:
  \cgather{
    a_0 \approx (1 -\alpha)/N
  }%
  Hence, we conclude, that at significance level $\geqq \alpha$, we have the bounds:
  \cgather{
    \mem{y}^Q e^{ \frac{\sqrt{8}N^2}{1-\alpha}\theta} \geqq Pr(x \rightarrow y) \geqq \mem{y}^Q e^{-\frac{\sqrt{8}N^2}{1-\alpha}\theta}
  }%
\end{proof}

\input{Tables/methods_summary}

\bibliographystyle{naturemag}
\bibliography{qbiome,allbib,tnet} 




\end{document}





% both the Social Security Administration and the Department of Veterans Affairs offer disability compensation for those with mental disorders, including PTSD. Individuals might fake PTSD for various reasons. Commonly, financial incentives are a motivator. For instance, U.S. veterans can receive significant yearly compensation from the Department of Veterans Affairs if they demonstrate PTSD linked to military service, creating an enticement to fake the condition. Similarly, the U.S. Social Security Administration provides disability payments for those with disorders like PTSD that hinder their work capacity, further encouraging some to fabricate PTSD. The potential for workers' compensation and personal injury lawsuits also provides reasons for individuals to falsely claim PTSD due to workplace incidents or other stressors.

% Some might fake PTSD to access inpatient hospital treatment or to lessen criminal responsibility and penalties in legal cases. Additionally, faking combat-related PTSD might be done to gain honor and recognition.

% The prevalence of faking PTSD varies depending on the context. In forensic evaluations, about 15.7\% might attempt to fake, compared to 7.4\% in non-forensic settings. In personal injury cases, an estimated 20-30\% of claimants might fake their symptoms. Among veterans seeking combat compensation, at least 20\% are suspected of fabricating their condition. In the criminal justice system, the rates of faking PTSD vary. Between 8\% and 17.4\% are suspected in competency to stand trial assessments, with a higher 45\%-56\% in incarcerated individuals seeking psychiatric services. These rates also correlate with the severity of the crime, with higher rates found among those charged with more severe crimes like murder and robbery.














% Post-traumatic stress disorder (PTSD) is an anxiety disorder that may develop after an individual experiences a traumatic event [REF]. In the United States, the Social Security Administration and the Department of Veterans Affairs each offer disability compensation programs that provide benefits for qualified individuals with mental disorders, including PTSD.  Individuals who malinger PTSD may have several motivations for doing so. First, financial incentives are common. For example, the Department of Veterans Affairs offers substantial annual financial compensation to U.S. veterans who can prove that they have PTSD related to their military service. This potential compensation can create an incentive for veterans to malinger PTSD.[3] Furthermore, the U.S. Social Security Administration offers social security disability payments to individuals documenting a disorder such as PTSD that impedes their ability to work, which additionally provides an incentive to malinger PTSD.[4] Additionally, the potential for workers compensation can motivate individuals reporting a traumatic event at their workplace to fabricate PTSD; and finally the potential for personal injury lawsuits can motivate someone to malinger PTSD and sue an individual for causing PTSD as a result of attack, accident or other stressor.[5]

% Some individuals are known to malinger PTSD to obtain inpatient hospital treatment.[6] Persons charged in criminal law cases are motivated to malinger PTSD in order to offset criminal responsibility for the crime or mitigate the associated penalties.[7] Some individuals are motivated to malinger PTSD (e.g., related to combat) in order to gain honor and recognition from others.[8]

% Prevalence[edit]
% The prevalence of malingering PTSD varies based on what one may be seeking. Differentiating between forensic and non-forensic evaluations, it has been found that malingering may be attempted in 15.7 percent of forensic evaluations and 7.4 percent of non-forensic evaluations.[9] As mentioned above, personal injury lawsuits can motivate someone to malinger PTSD. It is thought that between 20 and 30 percent of these people seeking settlements have malingered their PTSD results. It is also believed that a minimum of 20 percent of veterans seeking combat compensation have malingered.[10]

% Cases within the criminal justice system also vary. A malingering rate between 8 percent and 17.4 percent was found in subjects in competency to stand trial assessments. Of incarcerated subjects seeking psychiatric services, a much higher range between 45 percent and 56 percent were suspected to malinger. Malingering cases were also positively correlated with severity of the crimes for subjects in competency to stand trial assessments. Malingering rates for murderers and robbers are greater than double the rest of subjects seeking incompetency.[11]




% Malingering can lead to a decline in research and subsequent treatment for PTSD as it interferes with true studies. Insurance fraud may also come about through malingering, which hurts the economy~\cite{ali2015multimodal}. False PTSD diagnoses can adversely affect treatment planning, resource management, and research. The subjective nature of stressors, stereotypic presentation of symptoms, wealth of resources detailing how to malinger PTSD, and the high stakes for individuals involved in criminal, civil, and disability evaluations create challenges for making an accurate diagnosis. 


% Because of the substantial benefits available to individuals with a confirmed PTSD diagnosis, which causes occupational impairment, the distinct possibility of false diagnoses exist, some of which are due to malingering of PTSD. Malingering of PTSD consists of one feigning the disorder.


% Posttraumatic stress disorder (PTSD) can occur after a traumatic experience and can cause severe symptoms that interfere with a person's psychological, physical, interpersonal, occupational, and social functioning. It is important to accurately identify genuine cases of PTSD and, as part of the differential diagnosis, to rule out instances of false PTSD. False PTSD diagnoses can adversely affect treatment planning, resource management, and research. The subjective nature of stressors, stereotypic presentation of symptoms, wealth of resources detailing how to malinger PTSD, and the high stakes for individuals involved in criminal, civil, and disability evaluations create challenges for making an accurate diagnosis. T

% \IEEEPARstart{W}{hile} the precise definition of truth is a philosophically complex issue\cite{sep-truth}, as a practical matter it is clear that identification of truthful statements is foundational to every aspect of society, ranging from critical intelligence domains such as law enforcement and national security to medical domains such as mental health assessment and psychological evaluation. While historical approaches to lie detection have existed for thousands of years\cite{vicianova2015historical}, and even modern physiologically-based polygraph examinations date as far back as the early 20th century\cite{vicianova2015historical}, the scientific validity of such methods is questionable~\cite{national2003polygraph} and remains a topic of debate~\cite{Ganslen1990}. In clinical psychology and the criminal justice system, a specific form of deception known as malingering - the intentional manipulation of test results for personal gain~\cite{Rogers1997} - has been studied in some detail due to its deleterious consequences for the individual subject (e.g. misdiagnosis) and the broader community (e.g. misallocation of treatment or judicial resources)~\cite{rogers2008clinical}.  

% ,  include Facial Expression Analysis, which examines involuntary micro-expressions linked to deception~\cite{ekman1991who},  linguistic inconsistencies~\cite{mihalcea2009lie},  changes in heart rate and skin conductivity~\cite{verschuere2011memory}, thermal imaging to monitor heat patterns related to stress~\cite{pavlidis2002seeing}, behavioral Analysis observing body movements~\cite{burgoon2008cognitive}, and multimodal Approaches integrating multiple data sources for a comprehensive analysis~\cite{zhou2004automating}. These evolving AI methodologies, while promising, must be used cautiously considering ethical and privacy implications.


% Here, we implement a fundamentally novel  data-driven method for detecting deceptive or untruthful responses in structured interviews.  Conjecturing that response patterns generated from malingering and other forms of deception are probabilistically distinct from responses that are subversive or trying to mimic a mental disorder, we claim that deception may be detectable solely on the basis of responses themselves. The key insight here is that the responses themselves (and not the respondent's behavior when answering) are capable of signaling deception, even in cases where the respondent might possesses expert knowledge of how to answer the question. 

% Our algorithmic approach addresses limitations that may be present in other state-of-the-art methods while achieving performance at or beyond what has been reported to date. As an entirely response-driven method, we require no additional tests or resources to develop and deploy. Merging techniques from machine learning and information theory, we directly model the high-dimensional space of responses from all respondents, from which we are able to rigorously estimate the consistency of an individual respondent’s answers. % Our model-derived consistency measure - known as dissonance - gives a precise, quantitative method for identifying individuals whose responses may be reflective of malingering.
% Our findings suggest that this measure can reliably distinguish between actual and deceptive (even adversarial) responses on both population and individual levels, allowing us to identify subjects whose responses are inconsistent and indicative of possible deception.